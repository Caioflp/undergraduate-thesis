\chapter{NPIV Regression and Linear Inverse \\Problems}
\label{chap: npiv and lip}

Having defined what we mean by nonparametric instrumental variable (NPIV) regression, we now present two well established approaches, namely the ones in \cite{darolles2011} and \cite{newey2003}.
Our goal is to supply the reader with two solid examples of NPIV methods, so that they may serve as a reference for when we discuss our own method in Chapter \ref{chap: sagdiv}.
We start by connecting nonparametric instrumental variable regression with inverse problems.
% We will describe the idea behind Newey's seminal paper on NPIV \cite{newey2003}, which develops a nonparametric analog of 2SLS, and them move on to the more inverse problem (IP) oriented approach presented in \cite{florens2007} as well as \cite{darolles2011}.

\section{NPIV regression as an ill-posed linear inverse problem}
\label{sec: npiv and ill posed lip}

Recalling the notation presented in section \ref{sec: nonparametric}, we want to find $ \hstar \in L^{ 2 } ( X ) $ which satisfies
\begin{equation}
    \label{eq: basic equation}
    Y = \hstar ( X ) + \varepsilon
,\end{equation}
where $ \mean [ \varepsilon \mid Z ] = 0 $.
Letting $ r ( Z ) \defeq \mean [ Y \mid Z ] $ we have $ r \in L^{ 2 } ( Z ) $, so that (\ref{eq: basic equation}) is equivalent to
\begin{equation}
    \label{eq: linear equation}
    r = \meanop [ \hstar ]
,\end{equation}
where $ \meanop : L^{ 2 } ( X ) \to L^{ 2 } ( Z ) $ is the conditional expectation operator.
Assume that the joint distribution of $ ( X, Z ) $ is absolutely continuous with respect to Lebesgue measure in $ \R^{ d_{ X } + d_{ Z } } $, so that we may rewrite (\ref{eq: linear equation}) as
\begin{equation}
    \label{eq: expectation equation}
    r ( z ) = \mean [ \hstar ( X ) \mid Z = z ] = \int_{ \R^{ d_{ X } } } \hstar ( x ) p_{ X \mid Z } ( x \mid z ) \drm x
,\end{equation}
where $ p_{ X\mid Z } ( x \mid z ) $ is the conditional density of $ X $ given $ Z $.
This is a Fredholm integral equation of the first kind \cite{kress89} which, due to the nature of our problem, will most likely be ill-posed.
We dedicate some space to make this statement precise.

% {\color{red}Talk about the three ways in which the problem can be ill posed, relate them with identification, talk about compactness of $ \meanop $ and it being Hilbert-Schmidt, say what we are going to assume in what follows.}

Equation (\ref{eq: linear equation}) would describe a well-posed problem if the operator $ \meanop $ were invertible and the inverse $ \meanop^{ -1 } $, continuous.
There are three ways in which these conditions may be violated.
One of them was already discussed in subsection \ref{sec: identification}, the identification problem.
It corresponds to non-injectivity of $ \meanop $, that is, to $ \ker \meanop \neq \left\{ 0 \right\} $.
A possible correction for this problem is to look for the least norm solution.

Another violation happens if $ \meanop $ is not surjective, which leaves the possibility of $ r \notin \image ( \meanop ) $.
A way to bypass this difficulty is to project $ r $ onto the subspace $ \image ( \meanop ) $ of $ L^{ 2 } ( Z ) $ and find the inverse image of this projection.
However, the orthogonal projection onto a subspace is only well defined for $ \closure{\image ( \meanop )} $, since we need the subspace to be closed, which may not be the case for $ \image ( \meanop ) $.
Hence, we would need to assume that the projection of $ r $ onto $ \closure{\image ( \meanop )} $ belongs to $ \image ( \meanop ) $.
The methods we will analyze assume that the model (\ref{eq: basic equation}) is correctly specified and, therefore, that $ r \in \image ( \meanop ) $, so this will not be a concern for us.
The interested reader may consult \cite{florens2007} for ways to proceed without this assumption.

The last way for a problem such as (\ref{eq: basic equation}) to be ill-posed is to have $ \meanop $ injective and $ r \in \image ( \meanop ) $, but $ \meanop^{ -1 } : \image ( \meanop ) \to L^{ 2 } ( X ) $ discontinuous.
By the open mapping theorem \cite{rudin1991}, if $ \image ( \meanop ) $ is closed, then $ \meanop : L^{ 2 } ( X ) \to \image ( \meanop ) $ is an open map and the inverse is automatically continuous.
Hence, we must require that $ \image ( \meanop ) $ is not closed.
A prototypical example for this situation is when $ \meanop $ is a compact operator with infinite dimensional range.
If $ \image ( \meanop ) $ were closed, since $ \id_{ \image ( \meanop ) } = \meanop^{ -1 } \circ \meanop $ it would be a compact operator, which would imply compactness of the closed unit ball of $ \image ( \meanop ) $ and, hence, $ \dim \image ( \meanop ) < \infty $ \cite{florens2007}.

In light of this discussion, we now take the opportunity to study additional properties of the operator $ \meanop $, in particular, with the goal of providing a sufficient condition for it to be compact in terms of the distribution of $ X $ and $ Z $.
Notice that
\begin{align}
    \meanop [ h ] ( z )
    = \mean [ h ( X ) \mid Z = z ] 
    &= \int_{ \R^{ d_{ X } } } h ( x ) p_{ X \mid Z } ( x \mid z ) \drm x \nonumber \\
    &= \int_{ \R^{ d_{ X } } } h ( x ) \frac{ p_{ X, Z } ( x, z ) }{ p_{ X } ( x ) p_{ Z } ( z ) } p_{ X } ( x ) \drm x \nonumber \\
    &= \mean \left[
        h ( X ) \frac{ p_{ X, Z } ( X, z ) }{ p_{ X } ( X ) p_{ Z } ( z ) }
    \right] \nonumber \\
    &= \mean \left[ h ( X ) \Phi ( X, z ) \right] \label{eq: finding kernel}
.\end{align}
Hence, $ \meanop $ is an integral operator with kernel\footnote{Not to be confused with $ \ker \meanop $, a subspace of $ L^{ 2 } ( X ) $.}
\begin{equation*}
    \Phi ( x, z ) = \frac{ p_{ X, Z } ( x, z ) }{ p_{ X } ( x ) p_{ Z } ( z ) }
.\end{equation*}
Furthermore, since
\begin{align*}
    \dotprod{ \meanop[h], g }_{ L^{ 2 } ( Z ) }
    &= \mean \left[ \mean [ h ( X ) \mid Z ] g ( Z ) \right] \\
    &= \mean \left[ h ( X ) g ( Z ) \right] \\
    &= \mean \left[ h ( X ) \mean [ g ( Z ) \mid X ] \right] \\
    &= \dotprod{ h, \mean [ g ( Z ) \mid X = \cdot ] }_{ L^{ 2 } ( X ) }
,\end{align*}
by the defining equality of the adjoint operator $ \meanop^{ * } : L^{ 2 } ( Z ) \to L^{ 2 } ( X ) $ we have
\begin{equation*}
    \meanop^{ * } [ g ] ( X ) = \mean [ g ( Z ) \mid X ]
.\end{equation*}
Proceeding in a manner analogous to the one which got us (\ref{eq: finding kernel}), we may find that $ \meanop^{ * } $ is also an integral operator with the same kernel $ \Phi $ of $ \meanop $.

This observation is useful because, from \cite[theorem 2.34]{florens2007} for instance, we know that if the kernel $ \Phi $ satisfies
\begin{equation}
    \label{eq: Phi square integrable}
    \int_{ \R^{ d_{ X } } } \int_{ \R^{ d_{ Z } } } \Phi ( x, z )^2 p_{ X } ( x ) p_{ Z } ( z ) \drm z \ddrm x < \infty
,\end{equation}
then $ \meanop $ (and, consequently, $ \meanop^{ * } $) is a Hilbert-Schmidt operator and, in particular, compact.

It is also appropriate to note that in the case where $ X $ and $ Z $ share any coordinates, the operator $ \meanop $ \emph{cannot} be compact.
To see this, suppose that $ X = ( X_{ 1 }, W ) $ and $ Z = ( Z_{ 1 }, W ) $ for some random vector $ W $.
Denote by $ L^2 ( W ) $ the subspace of $ L^2 ( X ) $ (and $ L^2 ( Z ) $) of functions which depend only on $ W $.
Notice that, if $ f \in L^2 ( W ) $, then
\begin{equation*}
    \meanop [ f ] ( Z ) = \meanop [ f ] ( Z_{ 1 }, W ) = \mean [ f ( W ) \mid W, Z_{ 1 } ] = f ( W )
    \quad \text{i.e.}
    \quad \meanop [ f ] = f
.\end{equation*}
Hence, the image of the unit ball in $ L^2 ( X ) $ by $ \meanop $ contains the unit ball of $ L^2 ( W ) $.
Since the norm $ \norm{ \cdot }_{ L^2 ( Z ) } $ coincides with $ \norm{ \cdot }_{ L^2 ( W ) } $ in $ L^2 ( W ) $, and since $ L^2 ( W ) $ is closed as a subspace of $ L^2 ( Z ) $, then the unit ball in $ L^2 ( W ) $ is compact for $ \norm{ \cdot }_{ L^2 ( W ) } $ if and only if it is compact for $ \norm{ \cdot }_{ L^2 ( Z ) } $.
Then, since $ L^2 ( W ) $ is infinite dimensional, its unit ball is not compact, which means that the closure of image of the unit ball in $ L^2 ( X ) $ by $ \meanop $ is not compact and, hence, $ \meanop $ is not a compact operator.

We remark that, aside from requiring $ r \in \image ( \meanop ) $, we leave the possibility of any other form of ill-posedness to be present in our problem until stated otherwise.

\section{NPIV regression through Tikhonov regularization}
\label{sec: tikhonov}

In \cite{darolles2011}, the authors assume identification and also that the joint density $ p_{ X, Z } $ is dominated by the product of marginals $ p_{ x } \cdot p_{ Z } $.
This is the same as demanding the kernel $ \Phi $ to be bounded, which implies (\ref{eq: Phi square integrable}).
Hence, the operators $ \meanop $ and $ \meanop^{ * } $ are Hilbert-Schmidt.
Ill-posedness can then be characterized in terms of the singular values of $ \meanop $, which we will show next.

Since $ \meanop $ is compact, it has a countable \emph{singular value decomposition (SVD)} --- see \cite[section 15.4]{kress89} --- that is, there exists orthonormal sequences $ ( \varphi_{ n } )_{ n\in\N } $ and $ ( \psi_{ n } )_{ n\in\N } $, in $ L^2 ( X ) $ and $ L^2 ( Z ) $ respectively, as well as a decreasing sequence $ \lambda_{ 0 } \geq \lambda_{ 1 } \geq \dots \geq 0 $ of nonnegative real numbers (the singular values), such that
\begin{enumerate}[label=(\alph*)]
    \item The eigenvalues of $ \meanop^{ * } \meanop $ are precisely $ ( \lambda_{ i }^2 )_{ i \geq 0 } $ \label{en: singular values};
    \item $ \meanop [ \varphi_{ i } ] = \lambda_{ i } \psi_{ i } $ and $ \meanop^{ * } [ \psi_{ i } ] = \lambda_{ i } \varphi_{ i } $ for all $ i \geq 0 $;
    \item For all $ h \in L^2 ( X ) $ we have $ h = \sum_{ i=0 }^{ \infty } \dotprod{ h, \varphi_{ i } } \varphi_{ i } + \bar{ h } $, where $ \bar{ h } \in \ker \meanop $;
    \item For all $ g \in L^2 ( Z ) $ we have $ g = \sum_{ i=0 }^{ \infty } \dotprod{ g, \psi_{ i } } \psi_{ i } + \bar{ g } $, where $ \bar{ g } \in \ker \meanop^{ * } $;
\end{enumerate}
Therefore, we have
\begin{equation*}
    \meanop [ h ] = \sum_{ i=0 }^{ \infty } \lambda_{ i } \dotprod{ h, \varphi_{ i } } \psi_{ i }
\end{equation*}
as well as
\begin{equation*}
    \meanop^{ * } [ g ] = \sum_{ i=0 }^{ \infty } \lambda_{ i } \dotprod{ g, \psi_{ i } } \varphi_{ i }
.\end{equation*}
We can also see that, because of \ref{en: singular values}, identification is equivalent to all the $ \lambda_{ i } $ being strictly greater than $ 0 $.

Since $ \meanop^{ * } \meanop $ is compact, its sequence of eigenvalues $ ( \lambda_{ i }^2 )_{ i \geq 0 } $ and, hence, the sequence of singular values $ ( \lambda_{ i } )_{ i \geq 0 } $, must converge to $ 0 $.
This fact allows the NPIV regression problem to be ill posed when combined with the following proposition \cite[theorem 15.18]{kress89}, sometimes known as the Picard theorem:
\begin{prop}
    \label{prop: picard}
    We have $ r \in \image ( \meanop ) $ if, and only if, $ r \in ( \ker \meanop^{ * } )^{ \perp } $ and
    \begin{equation*}
        \sum_{ i=0 }^{ \infty } \frac{ 1 }{ \lambda_{ i }^2 } \dotprod{ r, \psi_{ i } }^2 < \infty
    .\end{equation*}
    Then, the solution to $ \meanop [ h ] = r $ is given by
    \begin{equation}
        \label{eq: closed form solution}
        h = \sum_{ i=0 }^{ \infty } \frac{ 1 }{ \lambda_{ i } } \dotprod{ r, \psi_{ i } } \varphi_{ i }
    .\end{equation}
\end{prop}

As we are assuming $ r \in \image ( \meanop ) $, this proposition tells us that if instead of $ r $ we measure $ r + \delta \psi_{ i } $, the solution will be perturbed by $ \frac{ \delta }{ \lambda_{ i } } \varphi_{ i } $, which may be very large in norm since $ \lambda_{ i } $ can be arbitrarily small.
To overcome this issue of noncontinuity, a classical technique in inverse problems is to apply Tikhonov regularization.

Observe that solving $ \meanop [ h ] = r $ is equivalent to solving
\begin{equation*}
    \argmin_{ h \in L^2 ( X ) } \norm{ \meanop [ h ] - r }_{ L^2 ( Z ) }^2
.\end{equation*}
Tikhonov regularization modifies this objective function by adding a penalization for large norm solutions.
Given a regularization parameter $ \alpha > 0 $, we define
\begin{equation*}
    L_{ \alpha } ( h ) \defeq \norm{ \meanop [ h ] - r }_{ L^2 ( Z ) }^2 + \alpha \norm{ h }_{ L^2 ( X ) }^2  
\end{equation*}
and the new optimization problem is 
\begin{equation*}
    \argmin_{ h \in L^2 ( X ) } L_{ \alpha } ( h )
.\end{equation*}
The function $ L_{ \alpha } $ is clearly convex and a straightforward application of the chain rule gives us
\begin{equation}
    \label{eq: loss gradient}
    \nabla L_{ \alpha } ( h ) = 2 \left( \meanop^{ * } [ \meanop [ h ] - r ] + \alpha h \right)
.\end{equation}
Hence, we can minimize $ L_{ \alpha } $ by setting the gradient to be null.
Denoting the solution by $ h_{ \alpha } $, we have:
\begin{align}
    2 ( \meanop^{ * } [ \meanop [ h_{ \alpha } ] - r ] + \alpha h_{ \alpha } ) = 0
    &\iff \meanop^{ * }\meanop [ h_{ \alpha } ] + \alpha h_{ \alpha } = \meanop^{ * } [ r ] \nonumber \\
    &\iff h_{ \alpha } = ( \meanop^{ * } \meanop + \alpha I )^{ -1 } \meanop^{ * } [ r ] \label{eq: tikhonov solution}
.\end{align}
Since the (eigenvector, eigenvalue) pairs of $ ( \meanop^{ * } \meanop + \alpha I )^{ -1 } $ are precisely $ ( 1 / ( \lambda_{ i }^2 + \alpha ), \varphi_{ i } ) $, a computation shows that
\begin{equation*}
    h_{ \alpha } = \sum_{ i=0 }^{ \infty } \frac{ \lambda_{ i } }{ \lambda_{ i }^2 + \alpha } \dotprod{ r, \psi_{ i } } \varphi_{ i }
.\end{equation*}
This clearly controls the decay of $ \lambda_{ i } $ by replacing $ 1/\lambda_{ i } $ in (\ref{eq: closed form solution}) with $ \lambda_{ i } / ( \lambda_{ i }^2 + \alpha ) $.

To be able to better control the convergence of $ h_{ \alpha } $ to $ \hstar $ as $ \alpha \to 0 $, the authors of \cite{darolles2011} impose a so-called \emph{source condition}, which states that there exists $ \beta > 0 $ such that
\begin{equation}
    \label{eq: source condition}
    \sum_{ i=0 }^{ \infty } \frac{ \dotprod{ \hstar, \varphi_{ i } }^2 }{ \lambda_{ i }^{ 2 \beta } } < \infty
\end{equation}
or, equivalently, that $ \hstar \in \image [ ( \meanop^{ * } \meanop )^{ \beta/2 } ] $.
This condition, although somewhat common in the inverse problems literature, is considerably restrictive, even more so as the degree of ill-posedness of the problem increases.
For example, if the $ \lambda_{ i } $ decay exponentially, then (\ref{eq: source condition}) requires the Fourier coefficients $ \dotprod{ \hstar, \varphi_{ i } } $ of $ \hstar $ to decay even more rapidly, which means that $ \hstar $ must have a highly precise finite-dimensional approximation.

Another technique employed in \cite{darolles2011} to obtain better consistency results is, instead of performing a standard Tikhonov regularization with (\ref{eq: tikhonov solution}), to build a sequence of estimators iteratively:
\begin{align}
    \begin{split}
        h_{ \alpha }^{ ( 1 ) } &= ( \meanop^{ * } \meanop + \alpha I )^{ -1 } \meanop^{ * } [ r ], \\
        h_{ \alpha }^{ ( k + 1 ) } &= ( \meanop^{ * } \meanop + \alpha I )^{ -1 } [ \meanop^{ * } r + h_{ \alpha }^{ ( k ) } ]
    .\end{split}
    \label{eq: iterated tikhonov}
\end{align}
While for the estimator (\ref{eq: tikhonov solution}) one has $ \norm{ \hstar - h_{ \alpha } } = \mathcal{O} ( \alpha^{ ( 2 \wedge \beta ) } ) $, for the iterated estimator it is possible to show $ \norm{ \hstar - h_{ \alpha }^{ ( k ) } } = \mathcal{O} ( \alpha^{ ( 2k \wedge \beta ) } ) $ \cite{darolles2011}.

To be able to apply the iteration scheme (\ref{eq: iterated tikhonov}), one must first obtain estimates for $ \meanop, \meanop^{ * } $ and $ r $, since these are unknown.
The choice was made to use Nadaraya-Watson kernel estimators --- see \cite{nadaraya64} and \cite{watson64}.

The important aspects of this method that the reader should keep in mind are the following:
\begin{itemize}
    \item Ill-posedness is tackled by assuming identification and performing (iterated) Tikhonov regularization;
    \item A source condition is necessary for providing convergence bounds;
    \item The estimation of conditional expectations is performed using Nadaraya-Watson kernels.
\end{itemize}


\section{Nonparametric 2SLS}

In \cite{newey2003}, the authors overcome the difficulties presented in Section \ref{sec: npiv and ill posed lip} by assuming identification and restricting the solution $ \hstar $ to belong to a compact set.
With this assumption, the problem of continuous inverse is automatically satisfied, since the inverse of a continuous function defined on a compact set and taking values in a Hausdorff space is automatically continuous \cite{munkres2000}.

However, we must advise the reader that the formulation of the nonparametric regression problem presented in \cite{newey2003} is a somewhat different then ours.
More specifically, the authors do not use the notation presented in subsection \ref{sec: problem specification}, neither the spaces $ L^2 ( X ) $ and $ L^2 ( Z ) $ for domain and codomain of the conditional expectation operator.
Therefore, the compact set where the solution is restricted to live is not necessarily a compact subset of $ L^{ 2 } ( X ) $, and it becomes difficult to compare both approaches from a theoretical perspective, a task which is left for future work.
Nonetheless, we decided to include a section on this paper because of its importance to the research in nonparametric methods for instrumental variables.
In the following, we will provide a high level description of their method, adapting the notation and focusing on the aspects we feel are the most important.
We refer the reader to the original paper for further details.

Suppose that the structural function can be approximated as follows:
\begin{equation}
    \label{eq: expansion on basis functions}
    \hstar ( x ) \approx \sum_{ j=1 }^{ J } \gamma_{ j } p_{ j } ( x )
,\end{equation}
where $ p_{ 1 }, p_{ 2 }, \dots $ is a sequence of ``basis'' functions and $ \gamma $ is the corresponding vector of coefficients.
Substituting this into (\ref{eq: expectation equation}), we get
\begin{equation}
    \label{eq: projected expansion}
    \mean [ Y \mid Z = z ] \approx r ( z ) = \sum_{ j=1 }^{ J } \gamma_{ j } \mean [ p_{ j } ( X ) \mid Z = z ]
.\end{equation}
This suggests a two stage procedure analogous to 2SLS.
In the first stage, a nonparametric estimate of $ \mean [ p_{ j } ( X ) \mid Z = z ] $ is obtained for $ j = 1, \dots, J $.
Then, in the second stage, the $ Y $ samples are regressed on these estimates to obtain the vector of coefficients $ \gamma $.
It is worth emphasizing that the second stage regression is substantially sensitive to the number $ J $ of approximating functions and the precision of the first stage estimators \cite{newey2003}.

More specifically, their search space consists of structural functions which take the following form:
\begin{equation}
    \label{eq: newey functions}
    h ( x ) = \beta^{ \trp } a ( x ) + h_{ 1 } ( x )
,\end{equation}
where $ \beta $ is a vector of unknown coefficients, $ a $ is a vector of known functions and $ h_{ 1 } $ is an unknown function.
The compactness property is obtained by placing bounds on $ \beta $ and restricting $ h_{ 1 } $ and its derivatives to be small in the tails, which is done by demanding a certain Sobolev norm of $ h_{ 1 } $ to be finite.
The details can be found in \cite{newey2003}.
It is important to notice that (\ref{eq: newey functions}) is a semiparametric model which allows $ h $ to be nonparametric in the center of the support of $ X $, but restricts it to be parametric in the tails.
Let us denote by $ \mathcal{G} $ the set of functions which are of the form (\ref{eq: newey functions}) and satisfy the mentioned regularity conditions.
It is assumed that $ \hstar = ( \beta^{ \star } )^{ \trp } a + \hstar_{ 1 }  \in \mathcal{G} $.

The nonparametric estimate for $ \hstar_{ 1 } $ is based on an expansion such as (\ref{eq: expansion on basis functions}), where the $ p_{ j } $ are chosen to be Hermite polynomials and the coefficients are restricted in a way that the final estimate belongs to $ \mathcal{G} $.

With these choices, it is not yet possible to then use equation (\ref{eq: projected expansion}) directly, since the conditional distribution of $ X $ given $ Z $ is not known.
Hence, a separate step is necessary, in which estimates for $ \mean [ Y \mid Z = z ] $, $ \mean [ a ( X ) \mid Z = z ] $ and $ \mean [ p_{ j } ( X ) \mid Z = z ] $ are obtained.
Assuming that these estimates are available and denoting them by $ \hat{ \mean } [ Y \mid Z = z ], \hat{ \mean } [ a ( X ) \mid Z = z ] $ and $ \hat{ \mean } [ p_{ j } ( X ) \mid Z = z ] $, equation (\ref{eq: expectation equation}) indicates that one should optimize the parameters in order to have
\begin{equation*}
    \hat{ \mean } [ Y \mid Z = z ] \approx \beta^{ \trp } \hat{ \mean } [ a ( X ) \mid Z = z ] + \sum_{ j=1 }^{ J } \gamma_{ j } \hat{ \mean } [ p_{ j } ( X ) \mid Z = z ]
.\end{equation*}
Therefore, the optimization objective chosen in \cite{newey2003} is
\begin{equation}
    \label{eq: newey loss}
    \widetilde{Q} ( \beta, \gamma )
    = \frac{ 1 }{ n } \sum_{ i=1 }^{ n } \left\{ Y_{ i } - \beta^{ \trp } \hat{ \mean } [ a ( X ) \mid Z = z_{ i } ] - \sum_{ j=1 }^{ J } \gamma_{ j } \hat{ \mean } [ p_{ j } ( X ) \mid Z = z_{ i } ] \right\}^2
.\end{equation}
In the paper, it is stated that it is not necessary to use $ \hat{ \mean } [ Y_{ i } \mid Z = z ] $ inside the objective instead of $ Y_{ i } $ because of the choice made for first stage $ \hat{ \mean } $-estimators, which we will shortly clarify.
With this objective function, the nonparametric 2SLS estimator is obtained minimizing $ \widetilde{Q} $ over $ ( \beta, \gamma ) $, that is,
\begin{equation}
    \label{eq: newey optimization problem}
    \hat{ h } ( x ) = \hat{ \beta }^{ \trp } a ( x ) + \sum_{ j=1 }^{ J } \hat{ \gamma }_{ j } p_{ j } ( x )
\end{equation}
where $ ( \hat{ \beta }, \hat{ \gamma } ) = \argmin \widetilde{Q} ( \beta, \gamma ) $ subject to $ h = \beta^{ \trp } a + \sum_{ j=1 }^{ J } \gamma_{ j } p_{ j } \in \mathcal{G} $.
This restriction is shown to be equivalent to a quadratic inequality restriction on $ \beta $ and $ \gamma $ and, therefore, there is a closed form solution for $ \hat{ \gamma } $ and $ \hat{ \beta } $.

What is left to do is to specify how the first stage estimators are computed.
For this task, a series estimator is employed, using splines or power series as approximating functions.
Then, the estimated values of $ \hat { \mean } [ a ( X ) \mid Z = z_{ i } ] $ and $ \hat{ \mean } [ p_{ j } ( X ) \mid Z = z_{ i } ] $ are employed in the computation of $ \widetilde{Q} ( \beta, \gamma ) $.
The specific form taken by these estimators may be consulted in \cite{newey2003}.

The key aspects of the nonparametric 2SLS (NP2SLS) estimator given by (\ref{eq: newey optimization problem}) which we would like the reader to have in mind are the following:
\begin{itemize}
    \item The ill-posedness of the inverse problem is solved by requiring identification and that the structural function $ \hstar $ belong to a compact set;
    \item There is a clear two stage procedure analogous to 2SLS, where the first stage estimates the conditional expectation using splines/power series and the second stage employs a truncated basis expansion with Hermite polynomials.
\end{itemize}

To end this section, we remark that although \cite{newey2003} provides consistency results of the form $ \norm{ \hat{ h } - \hstar } \to 0 $ (in probability for a norm $ \norm{ \cdot } $ related to the space $ \mathcal{G} $), these results rest on non-trivial assumptions about the quality of the first stage regression, the denseness of the chosen basis functions and the already mentioned compactness of the parameter set.
% Our estimator for $ h^{ \star }_{ 1 } $ will be a finite dimensional approximation very similar to (\ref{eq: expansion on basis functions}).
% The only modification is that we standardize $ x $ using the sample $ ( X_{ i } )_{ i=1 }^{ N } $.
% Let $ \hat{ \mu } $ and $ \hat{ \Sigma } $ denote the sample mean and covariance matrix for $ X $.
% Let $ \hat{ x } = \hat{ \Sigma }^{ -1/2 } ( x - \hat{ \mu } ) $.
% We consider a Hermite polynomial approximation for $ \hstar_{ 1 } $:
% \begin{equation*}
%     h_{ 1 } ( x ) = \sum_{ i=1 }^{ J } \gamma_{ j } p_{ j } ( \hat{ x } ),
%     \quad
%     p_{ j } ( x ) = \exp \left\{ - x^{ \trp } x \right\} \cdot x^{ \lambda ( j ) }
% ,\end{equation*}
% where $ \lambda ( j ) \in \Z_{ \geq 0 }^{ d_{ x } } $ is a multi-index and
% \begin{equation*}
%     x^{ \lambda ( j ) } = \prod_{ k=1 }^{ d_{ X } } x_{ k }^{ \lambda ( j )_{ k } }
% .\end{equation*}
% We require $ \abs{ \lambda ( j ) } = \lambda ( j )_{ 1 } + \cdots + \lambda ( j )_{ d_{ X } } $ to be increasing in $ j $.
% 
% In order to have the estimator $ h = \beta^{ \trp } a + h_{ 1 } $ belong to $ \mathcal{G} $, we must impose restrictions on $ \beta $ and $ \gamma $.
% It is shown in \cite{newey2003} that the necessary restrictions are a norm bound on $ \beta $ and a quadratic inequality restriction on $ \gamma $.

\section{Problem setup}

\subsection{Basic definitions}

Fix a probability space $ ( \Omega, \mathcal{A}, \prob ) $.
Given $ X \in L^{ 2 } ( \Omega, \mathcal{A}, \prob ; \mathcal{X} \subseteq \R^{ p } ) $, we define
\begin{equation*}
    L^{ 2 } ( X ) \defeq \left\{ h : \mathcal{X} \to \R \ : \ \mean [ h ( X )^2 ] < \infty \right\}
,\end{equation*}
that is, $ L^{ 2 } ( X ) = L^{ 2 } ( \mathcal{X}, \mathcal{B} ( \mathcal{X} ), \nu_{ X } ) $\footnote{We denote by $ \nu_{ X } $ the distribution of the r.v. $ X $ and by $ \mathcal{B} ( \mathcal{X} ) $ the Borel $ \sigma $-algebra in $ \mathcal{X} $.}, a Hilbert space equipped with the inner product $ \dotprod{ h, g }_{ L^{ 2 } ( X ) } = \mean [ h ( X ) g ( X ) ] $.
The regression problem we are interested in has the form
\begin{equation}
    \label{eq: main problem}
    Y = \hstar (X) + \varepsilon
,\end{equation}
where $ \hstar \in L^{ 2 } ( X ) $ and $ \varepsilon $ is an square-integrable r.v. such that $ \mean [ \varepsilon \mid X ] \neq 0 $.
We assume there exists $ Z \in L^{ 2 } ( \Omega, \mathcal{A}, \prob ; \mathcal{Z} \subseteq \R^{ q } ) $ such that
\begin{enumerate}[label=\roman*)]
    \item $ Z $ influences $ X $, that is, $ \nu_{ X|Z } ( \cdot \mid Z ) \neq \nu_{ X } ( \cdot ) $;
    \item $ Z $ influences $ Y $ only through $ Z $;
    \item $ Z $ and $ \varepsilon $ are uncorrelated, that is, $ \mean [ \varepsilon \mid Z ] = 0 $.
\end{enumerate}
The space $ L^{ 2 } ( Z ) $ is defined accordingly.
This variable is called the \emph{instrumental variable}.
The problem consists of estimating $ \hstar $ based on independent joint samples from $ X, Z $ and $ Y $.

Conditioning (\ref{eq: main problem}) in $ Z $, we find
\begin{equation}
    \label{eq: problem given Z}
    \mean [ Y \mid Z ] = \mean [ \hstar ( X ) \mid Z ]
.\end{equation}
This motivates us to introduce the operator $ \meanop : L^{ 2 } ( X ) \to L^{ 2 } ( Z ) $ defined by
\begin{equation*}
    \meanop [ h ] ( z ) \defeq \mean [ h ( X ) \mid Z = z ]
.\end{equation*}
Clearly $ \meanop $ is linear and, using Jensen's inequality, one may prove that it's bounded.
It's also interesting to notice that its adjoint $ \meanop^{ * } : L^{ 2 } ( Z ) \to L^{ 2 } ( X ) $ satisfies
\begin{equation}
    \label{eq: T adjoint}
    \meanop^{ * } [ g ] ( x ) = \mean [ g ( Z ) \mid X = x ].
\end{equation}
Define $ r_{ 0 } : \mathcal{Z} \to \R $ by $ r_{ 0 } ( Z ) = \mean [ Y \mid Z ] $.
Again by Jensen's inequality, we have $ r_{ 0 } \in L^{ 2 } ( Z ) $, and thus we can rewrite (\ref{eq: problem given Z}) as
\begin{equation}
    \label{eq: problem with T}
    \meanop [ \hstar ] = r_{ 0 }
.\end{equation}
Hence, (\ref{eq: main problem}) can be formulated as an inverse problem, where we wish to invert the operator $ \meanop $.
\improvement{Discuss the other implication, that if $ h $ satisfies $ \meanop [ h ] = r_{ 0 } $, then $ h = \hstar $.
This is false, but the reason can be connected to the strength of the instrument $ Z $.}

\subsection{Risk measure}

Let $ \ell : \R \times \R \to \R $ be a pointwise loss function, which, with respect to its second argument, is convex and differentiable.
We use the symbol $ \partial_{ 2 } $ to denote a derivative with respect to the second argument.
The example to keep in mind is the quadratic loss function $ \ell ( y, y' ) = \frac{ 1 }{ 2 } ( y - y' )^2 $.
Given $ h \in L^{ 2 } ( X ) $, we define the \emph{populational risk} associated with it to be
\begin{equation*}
    \risk ( h ) \defeq \mean [ \ell ( r_{ 0 } ( Z ), \meanop h ( Z ) ) ]
.\end{equation*}
We would like to solve
\begin{equation*}
    \inf_{ h \in \mathcal{F} } \risk ( h )
,\end{equation*}
where $ \mathcal{F} \subseteq L^{ 2 } ( X ) $ is a bounded, closed, convex set such that $ \hstar \in \mathcal{F} $ \info{Assumption}.

We now state all the assumptions needed about the function $ \ell $ for future reference:
\begin{assumption}[Regularity of $ \ell $]
    \label{assumption loss}
    \begin{enumerate}
        \item[]
        \item The function $ \ell : \R \times \R \to \R $ is convex and $ C^2 $ with respect to its second argument;
        \item There exists $ \theta_{ 0 } > 0 $ such that for all $ f, g \in L^2 ( X ) $
            \begin{equation}
                \label{eq: loss regularity}
                \sup_{ \abs{ \theta } < \theta_{ 0 } }
                \mean \left[
                    \partial_{ 2 }^2 \ell ( r_{ 0 } ( Z ), \meanop [ g + \theta f ] ( Z ) ) \cdot \meanop [ f ] ( Z )^2
                \right] < \infty
            ;\end{equation}
            \label{en: loss regularity}
    \end{enumerate}
\end{assumption}
Assumption \ref{assumption loss}.\ref{en: loss regularity} is a mild integrability condition which can be easily shown to hold in the quadratic case.

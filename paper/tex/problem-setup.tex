\section{Problem setup}

\subsection{Basic definitions}

Fix a probability space $ ( \Omega, \mathcal{A}, \prob ) $.
Given $ X \in L^{ 2 } ( \Omega, \mathcal{A}, \prob ; \mathcal{X} \subseteq \R^{ p } ) $, we define
\begin{equation*}
    L^{ 2 } ( X ) \defeq \left\{ h : \mathcal{X} \to \R \ : \ \mean [ h ( X )^2 ] < \infty \right\}
,\end{equation*}
that is, $ L^{ 2 } ( X ) = L^{ 2 } ( \mathcal{X}, \mathcal{B} ( \mathcal{X} ), \nu_{ X } ) $, where we denote by $ \nu_{ X } $ the distribution of the r.v. $ X $ and by $ \mathcal{B} ( \mathcal{X} ) $ the Borel $ \sigma $-algebra in $ \mathcal{X} $.
This is a Hilbert space equipped with the inner product $ \dotprod{ h, g }_{ L^{ 2 } ( X ) } = \mean [ h ( X ) g ( X ) ] $.
The regression problem we are interested in has the form
\begin{equation}
    \label{eq: main problem}
    Y = \hstar (X) + \varepsilon
,\end{equation}
where $ \hstar \in L^{ 2 } ( X ) $ and $ \varepsilon $ is an square-integrable r.v. such that $ \mean [ \varepsilon \mid X ] \neq 0 $.
We assume there exists $ Z \in L^{ 2 } ( \Omega, \mathcal{A}, \prob ; \mathcal{Z} \subseteq \R^{ q } ) $ such that
\begin{enumerate}[label=\roman*)]
    \item $ Z $ influences $ X $, that is, $ \nu_{ X|Z } ( \cdot \mid Z ) \neq \nu_{ X } ( \cdot ) $;
    \item $ Z $ influences $ Y $ only through $ Z $;
    \item $ Z $ and $ \varepsilon $ are uncorrelated, that is, $ \mean [ \varepsilon \mid Z ] = 0 $.
\end{enumerate}
The space $ L^{ 2 } ( Z ) = L^{ 2 } ( \mathcal{Z}, \mathcal{B} ( \mathcal{Z} ), \nu_{ Z } ) $ is defined accordingly.
This variable is called the \emph{instrumental variable}.
The problem consists of estimating $ \hstar $ based on independent joint samples from $ X, Z $ and $ Y $.

Conditioning (\ref{eq: main problem}) in $ Z $, we find
\begin{equation}
    \label{eq: problem given Z}
    \mean [ Y \mid Z ] = \mean [ \hstar ( X ) \mid Z ]
.\end{equation}
This motivates us to introduce the operator $ \meanop : L^{ 2 } ( X ) \to L^{ 2 } ( Z ) $ defined by
\begin{equation*}
    \meanop [ h ] ( z ) \defeq \mean [ h ( X ) \mid Z = z ]
.\end{equation*}
Clearly $ \meanop $ is linear and, using Jensen's inequality, one may prove that it's bounded.
It's also interesting to notice that its adjoint $ \meanop^{ * } : L^{ 2 } ( Z ) \to L^{ 2 } ( X ) $ satisfies
\begin{equation}
    \label{eq: T adjoint}
    \meanop^{ * } [ g ] ( x ) = \mean [ g ( Z ) \mid X = x ].
\end{equation}
Define $ r_{ 0 } : \mathcal{Z} \to \R $ by $ r_{ 0 } ( Z ) = \mean [ Y \mid Z ] $.
Again by Jensen's inequality, we have $ r_{ 0 } \in L^{ 2 } ( Z ) $, and thus we can rewrite (\ref{eq: problem given Z}) as
\begin{equation}
    \label{eq: problem with T}
    \meanop [ \hstar ] = r_{ 0 }
.\end{equation}
Hence, (\ref{eq: main problem}) can be formulated as an inverse problem, where we wish to invert the operator $ \meanop $.
\improvement{Discuss the other implication, that if $ h $ satisfies $ \meanop [ h ] = r_{ 0 } $, then $ h = \hstar $.
This is false, but the reason can be connected to the strength of the instrument $ Z $.}

\subsection{Risk measure}

Let $ \ell : \R \times \R \to \R $ be a pointwise loss function, which, with respect to its second argument, is convex and differentiable.
We use the symbol $ \partial_{ 2 } $ to denote a derivative with respect to the second argument.
The example to keep in mind is the quadratic loss function $ \ell ( y, y' ) = \frac{ 1 }{ 2 } ( y - y' )^2 $.
Given $ h \in L^{ 2 } ( X ) $, we define the \emph{populational risk} associated with it to be
\begin{equation*}
    \risk ( h ) \defeq \mean [ \ell ( r_{ 0 } ( Z ), \meanop [ h ] ( Z ) ) ]
.\end{equation*}
We would like to solve
\begin{equation*}
    \inf_{ h \in \mathcal{F} } \risk ( h )
,\end{equation*}
where $ \mathcal{F} \subseteq L^{ 2 } ( X ) $ is a bounded, closed, convex set such that $ \hstar \in \mathcal{F} $ \info{Assumption}.
We also assume that $ D \defeq \diam \mathcal{F} < \infty $ and that $ 0 \in \mathcal{F} $, so that $ \norm{ h } \leq D $ if $ h \in \mathcal{F} $.
A possible choice for the set $ \mathcal{F} $ is
\begin{equation*}
    \mathcal{F} = \left\{ h \in L^{ 2 } ( X ) : \norm{ h }_{ \infty } \leq A \right\}
,\end{equation*}
where $ A > 0 $ is a constant chosen \emph{a priori}.
This set is obviously closed, convex and bounded in the $ L^2 ( X ) $ norm.
Furthermore, the projection operator $ \pi_{ \mathcal{F} } $ is very easy to compute, as $ \pi_{ \mathcal{F} } [ h ] $ is obtained by cropping $ h $ inside $ [ -A, A ] $.
More formally,
\begin{equation*}
    \pi_{ \mathcal{F} } [ h ] = h^{ + } \wedge A - h^{ - } \wedge A
.\end{equation*}

We now state all the assumptions needed about the function $ \ell $:
\begin{assumption}[Regularity of $ \ell $]
    \label{assumption loss}
    \begin{enumerate}
        \item[]
        \item The function $ \ell : \R \times \R \to \R $ is convex and $ C^2 $ with respect to its second argument;
        \item The function $ \ell $ has Lipschitz first derivative with respect to the second argument, i.e., there exists $ L \geq 0 $ such that, for all $ y, y', u, u' \in \R $ we have
            \begin{equation*}
                \abs{ \partial_{ 2 } \ell ( y, y' ) - \partial_{ 2 } \ell ( u, u' ) }
                \leq L ( \abs{ y - u } + \abs{ y' - u' } )
            .\end{equation*}
            \label{en: lipschitz gradients}
    \end{enumerate}
\end{assumption}
Some useful facts which follow immediately from these assumptions are:
\begin{proposition}
    \label{prop: loss properties}
    Under Assumption \ref{assumption loss} we have:
    \begin{enumerate}
        \item \label{bounded growth} Setting $ C_{ 0 } = \abs{ \partial_{ 2 } \ell ( 0, 0 ) } $ we have
            \begin{equation*}
                \abs{ \partial_{ 2 } \ell ( y, y' ) } \leq C_{ 0 } + L ( \abs{ y } + \abs{ y' } )
            \end{equation*}
            for all $ y, y' \in \R $;
        \item \label{continuous composition} The map $ f \mapsto \partial_{ 2 } \ell ( r_{ 0 } ( \cdot ), f ( \cdot ) ) $ from $ L^{ 2 } ( Z ) $ to $ L^{ 2 } ( Z ) $ is well-defined and $ L $-Lipschitz.
        \item \label{bounded second derivative} The second derivative with respect to the second argument is bounded: $ \abs{ \partial_{ 2 }^{ 2 } \ell ( y, y' ) } \leq L $ for all $ y, y' \in \R $;
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}
        \item[]
        \item Write $ \partial_{ 2 } \ell ( y, y' ) = \partial_{ 2 } \ell ( y, y' ) - \partial_{ 2 } \ell ( 0, 0 ) + \partial_{ 2 } \ell ( 0, 0 ) $ and apply the triangle inequality as well as Assumption \ref{assumption loss}.\ref{en: lipschitz gradients}.
        \item From the previous item we know this map is well-defined.
            If $ f $ and $ g $ belong to $ L^{ 2 } ( Z ) $, we have
            \begin{align*}
                \norm{ \partial_{ 2 } \ell ( r_{ 0 } ( \cdot ), f ( \cdot ) ) - \partial_{ 2 } \ell ( r_{ 0 } ( \cdot ), g ( \cdot ) ) }_{ L^2 ( Z ) }^2
                &= \mean \left[
                    \abs{ 
                        \partial_{ 2 } ( r_{ 0 } ( Z ), f ( Z ) )
                        - \partial_{ 2 } ( r_{ 0 } ( Z ), g ( Z ) )
                    }^2
                \right] \\
                &\leq L^2 \mean \left[
                    \abs{ f ( Z ) - g ( Z ) }^2
                \right] \\
                &= L^2 \norm{ f - g }_{ L^2 ( Z ) }^2
            .\end{align*}
        \item Follows from the definition of derivative and Assumption \ref{assumption loss}.\ref{en: lipschitz gradients}.
    \end{enumerate}
\end{proof}
